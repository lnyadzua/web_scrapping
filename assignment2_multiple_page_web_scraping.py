# -*- coding: utf-8 -*-
"""assignment 2: multiple-page web scraping .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JAipyp8jYzJ_MG8A8HbH0r5VbCk99TJW
"""

!pip install requests
!pip install beautifulsoup4

from bs4 import BeautifulSoup
import requests

import pandas as pd
import csv
import time

books = []
print("Starting book scraping...")

for i in range(1,50):
  url = f"https://books.toscrape.com/catalogue/page-{i}.html"
  print(f"Scraping for pages{i}...")

  try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # Raise exception for HTTP errors
        soup = BeautifulSoup(response.content, 'html.parser')

        # find all books articles

        ol = soup.find('ol')
        articles = ol.find_all('article', class_='product_pod')
        print(f"Found {len(articles)} books on page {i}")

        for index in range(len(articles)):
            article = articles[index]
            try:
                print(f"Processing book {index + 1} on page {i}...")  # +1 to start counting from 1

                # Extract title
                image = article.find('img')
                title = image.attrs['alt'].strip()

                # Extract price
                price = article.find('p', class_='price_color').text.strip()
                price = float(price[1:])  # Remove '£' and convert to float

                # Extract availability
                availability = article.find('p', class_='instock availability').text.strip()

                #Extract using star rating
                startag = article.find('p', class_='star-rating')
                star = startag['class'][1]

                if star not in ['One','Three','Five']:
                    print(f"skip book has {star}.stars")
                    continue


                # Append cleaned data
                books.append([title, price, availability])
                print(f"Book scraped: {title}, £{price}, {availability}")


            except Exception as e:
                print(f"Skipped book {index + 1} due to error: {e}")
                continue

        # Pause before next request
        print("Waiting 5 second before the next page...")
        time.sleep(5)

  except Exception as e:
        print(f"Could not scrape page {i} due to error: {e}")
        continue



# Save data to CSV
print("Saving data to CSV...")
csv_file = 'my_books.csv'
df = pd.DataFrame(books, columns=['Title', 'Price', 'Availability'])
df.to_csv(csv_file, index=False)

print(f"Scraping complete. {len(books)} books saved to '{csv_file}'.")